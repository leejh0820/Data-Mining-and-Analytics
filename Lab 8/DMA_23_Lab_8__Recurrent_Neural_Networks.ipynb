{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kTKaVK47knP"
      },
      "source": [
        "# Lab 8 Neural Language Model\n",
        "A language model predicts the next word in the sequence based on the specific words that have come before it in the sequence.\n",
        "\n",
        "It is also possible to develop language models at the character level using neural networks. The benefit of character-based language models is their small vocabulary and flexibility in handling any words, punctuation, and other document structure. This comes at the cost of requiring larger models that are slower to train.\n",
        "\n",
        "Nevertheless, in the field of neural language models, character-based models offer a lot of promise for a general, flexible and powerful approach to language modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK2Ew8_C7knQ"
      },
      "source": [
        "As a prerequisite for the lab, make sure to pip install:\n",
        "- keras\n",
        "- tensorflow\n",
        "- h5py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIs5eFI97knR"
      },
      "source": [
        "# Source Text Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQku8-L97knT"
      },
      "source": [
        "To start out with, we'll be using a simple nursery rhyme. It's quite short so we can actually train something on your CPU and see relatively interesting results. Please copy and paste the following text in a text file and save it as \"rhymes.txt\". Place this in the same directory as this jupyter notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wl1ZLGG7wii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "629562e8-3983-4621-c8ae-b6ebddb89175"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install h5py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqQdG7M68HVt"
      },
      "source": [
        "\n",
        "s='Sing a song of sixpence,\\\n",
        "A pocket full of rye.\\\n",
        "Four and twenty blackbirds,\\\n",
        "Baked in a pie.\\\n",
        "When the pie was opened\\\n",
        "The birds began to sing;\\\n",
        "Wasnâ€™t that a dainty dish,\\\n",
        "To set before the king.\\\n",
        "The king was in his counting house,\\\n",
        "Counting out his money;\\\n",
        "The queen was in the parlour,\\\n",
        "Eating bread and honey.\\\n",
        "The maid was in the garden,\\\n",
        "Hanging out the clothes,\\\n",
        "When down came a blackbird\\\n",
        "And pecked off her nose.'\n",
        "\n",
        "with open('rhymes.txt','w') as f:\n",
        "  f.write(s)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq7wq3fF7knV"
      },
      "source": [
        "    Sing a song of sixpence,\n",
        "    A pocket full of rye.\n",
        "    Four and twenty blackbirds,\n",
        "    Baked in a pie.\n",
        "\n",
        "    When the pie was opened\n",
        "    The birds began to sing;\n",
        "    Wasnâ€™t that a dainty dish,\n",
        "    To set before the king.\n",
        "\n",
        "    The king was in his counting house,\n",
        "    Counting out his money;\n",
        "    The queen was in the parlour,\n",
        "    Eating bread and honey.\n",
        "\n",
        "    The maid was in the garden,\n",
        "    Hanging out the clothes,\n",
        "    When down came a blackbird\n",
        "    And pecked off her nose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdOJmdjU7knW"
      },
      "source": [
        "# Sequence Generation\n",
        "\n",
        "A language model must be trained on the text, and in the case of a character-based language model, the input and output sequences must be characters.\n",
        "\n",
        "The number of characters used as input will also define the number of characters that will need to be provided to the model in order to elicit the first predicted character.\n",
        "\n",
        "After the first character has been generated, it can be appended to the input sequence and used as input for the model to generate the next character.\n",
        "\n",
        "Longer sequences offer more context for the model to learn what character to output next but take longer to train and impose more burden on seeding the model when generating text.\n",
        "\n",
        "We will use an arbitrary length of 10 characters for this model.\n",
        "\n",
        "There is not a lot of text, and 10 characters is a few words.\n",
        "\n",
        "We can now transform the raw text into a form that our model can learn; specifically, input and output sequences of characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjJfvv1U7knY"
      },
      "source": [
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6GbMmMs7knb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "09ce8f1b-bbe8-4710-c34b-ddb86cd94771"
      },
      "source": [
        "#load text\n",
        "raw_text = load_doc('rhymes.txt')\n",
        "print(raw_text)\n",
        "\n",
        "# clean\n",
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "\n",
        "# organize into sequences of characters\n",
        "length = 10\n",
        "sequences = list()\n",
        "for i in range(length, len(raw_text)):\n",
        "    # select sequence of tokens\n",
        "    seq = raw_text[i-length:i+1]\n",
        "    # store\n",
        "    sequences.append(seq)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sing a song of sixpence,A pocket full of rye.Four and twenty blackbirds,Baked in a pie.When the pie was openedThe birds began to sing;Wasnâ€™t that a dainty dish,To set before the king.The king was in his counting house,Counting out his money;The queen was in the parlour,Eating bread and honey.The maid was in the garden,Hanging out the clothes,When down came a blackbirdAnd pecked off her nose.\n",
            "Total Sequences: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBepEyDn7kne"
      },
      "source": [
        "# save sequences to file\n",
        "out_filename = 'char_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohjVbv1l7kng"
      },
      "source": [
        "# Train a Model\n",
        "In this section, we will develop a neural language model for the prepared sequence data.\n",
        "\n",
        "The model will read encoded characters and predict the next character in the sequence. A Long Short-Term Memory recurrent neural network hidden layer will be used to learn the context from the input sequence in order to make the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HBgtQvY7knh"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ruXoPS7knl"
      },
      "source": [
        "# load\n",
        "\n",
        "in_filename = 'char_sequences.txt'\n",
        "raw_text = load_doc(in_filename)\n",
        "lines = raw_text.split('\\n')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTnvoY-f7kno"
      },
      "source": [
        "The sequences of characters must be encoded as integers.This means that each unique character will be assigned a specific integer value and each sequence of characters will be encoded as a sequence of integers. We can create the mapping given a sorted set of unique characters in the raw input data. The mapping is a dictionary of character values to integer values.\n",
        "\n",
        "Next, we can process each sequence of characters one at a time and use the dictionary mapping to look up the integer value for each character. The result is a list of integer lists.\n",
        "\n",
        "We need to know the size of the vocabulary later. We can retrieve this as the size of the dictionary mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMndzt5v7kno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c6146908-9d18-4046-e210-49fda4253037"
      },
      "source": [
        "# integer encode sequences of characters\n",
        "chars = sorted(list(set(raw_text)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "sequences = list()\n",
        "for line in lines:\n",
        "    # integer encode line\n",
        "    encoded_seq = [mapping[char] for char in line]\n",
        "    # store\n",
        "    sequences.append(encoded_seq)\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qyCICfV7knx"
      },
      "source": [
        "The model is defined with an input layer that takes sequences that have 10 time steps and 38 features for the one hot encoded input sequences. Rather than specify these numbers, we use the second and third dimensions on the X input data. This is so that if we change the length of the sequences or size of the vocabulary, we do not need to change the model definition.\n",
        "\n",
        "The model has a single LSTM hidden layer with 75 memory cells. The model has a fully connected output layer that outputs one vector with a probability distribution across all characters in the vocabulary. A softmax activation function is used on the output layer to ensure the output has the properties of a probability distribution.\n",
        "\n",
        "The model is learning a multi-class classification problem, therefore we use the categorical log loss intended for this type of problem. The efficient Adam implementation of gradient descent is used to optimize the model and accuracy is reported at the end of each batch update. The model is fit for 50 training epochs.\n",
        "\n",
        "# To Do:\n",
        "- Try different numbers of memory cells\n",
        "- Try different types and amounts of recurrent and fully connected layers\n",
        "- Try different lengths of training epochs\n",
        "- Try different sequence lengths and pre-processing of data\n",
        "- Try regularization techniques such as Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "K1jO9lYX7kny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2497516e-7b23-42b6-a722-23f810c705ac"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "history=model.fit(X, y, epochs=100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 75)                34200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 38)                2888      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37088 (144.88 KB)\n",
            "Trainable params: 37088 (144.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 3s 14ms/step - loss: 3.6142 - accuracy: 0.0547\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 3.5113 - accuracy: 0.1042\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 3.2211 - accuracy: 0.1302\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 3.1188 - accuracy: 0.1589\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 3.0560 - accuracy: 0.1589\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 3.0422 - accuracy: 0.1589\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 3.0255 - accuracy: 0.1589\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 3.0066 - accuracy: 0.1589\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.9948 - accuracy: 0.1589\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.9750 - accuracy: 0.1797\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.9633 - accuracy: 0.1615\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.9355 - accuracy: 0.1797\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.9049 - accuracy: 0.1979\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.8841 - accuracy: 0.1927\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 2.8456 - accuracy: 0.2057\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 2.8125 - accuracy: 0.2031\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.7806 - accuracy: 0.2318\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.7440 - accuracy: 0.2214\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 2.7064 - accuracy: 0.2396\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 2.6773 - accuracy: 0.2214\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.6573 - accuracy: 0.2292\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.6011 - accuracy: 0.2552\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 2.5800 - accuracy: 0.2500\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.5437 - accuracy: 0.2865\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.5086 - accuracy: 0.2734\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.4775 - accuracy: 0.2839\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 2.4426 - accuracy: 0.2969\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.4112 - accuracy: 0.3073\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 2.3683 - accuracy: 0.3516\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.3093 - accuracy: 0.3255\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 2.3030 - accuracy: 0.3333\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 2.2421 - accuracy: 0.3698\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 2.2230 - accuracy: 0.3880\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 2.1929 - accuracy: 0.3724\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 2.1585 - accuracy: 0.3880\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.0983 - accuracy: 0.4219\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 2.0766 - accuracy: 0.3984\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2.0087 - accuracy: 0.4453\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 1.9732 - accuracy: 0.4583\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.9427 - accuracy: 0.4557\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.8997 - accuracy: 0.4844\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.8401 - accuracy: 0.4740\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.8084 - accuracy: 0.4974\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 1.7705 - accuracy: 0.4974\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 1.7345 - accuracy: 0.5339\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6869 - accuracy: 0.5469\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.6449 - accuracy: 0.5521\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.6077 - accuracy: 0.5755\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 1.5649 - accuracy: 0.5729\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.5392 - accuracy: 0.5859\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.4958 - accuracy: 0.6068\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.4533 - accuracy: 0.6120\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.4292 - accuracy: 0.6172\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 1.3879 - accuracy: 0.6510\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.3568 - accuracy: 0.6615\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.3368 - accuracy: 0.6823\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.2745 - accuracy: 0.7031\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.2437 - accuracy: 0.7057\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.1991 - accuracy: 0.7240\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1650 - accuracy: 0.7214\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.1350 - accuracy: 0.7448\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 1.1092 - accuracy: 0.7604\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.0639 - accuracy: 0.7760\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0164 - accuracy: 0.7995\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.0018 - accuracy: 0.7865\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.9628 - accuracy: 0.7995\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.9377 - accuracy: 0.8255\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.9048 - accuracy: 0.8203\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.8598 - accuracy: 0.8255\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.8350 - accuracy: 0.8516\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.8036 - accuracy: 0.8516\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7788 - accuracy: 0.8620\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.7516 - accuracy: 0.8724\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7232 - accuracy: 0.8906\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7020 - accuracy: 0.8750\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6910 - accuracy: 0.8880\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6543 - accuracy: 0.9036\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6191 - accuracy: 0.9219\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5894 - accuracy: 0.9401\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5765 - accuracy: 0.9401\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5595 - accuracy: 0.9401\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.5583 - accuracy: 0.9245\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.5266 - accuracy: 0.9427\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5078 - accuracy: 0.9427\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.4911 - accuracy: 0.9505\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.4630 - accuracy: 0.9557\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.4572 - accuracy: 0.9688\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.4349 - accuracy: 0.9661\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.4185 - accuracy: 0.9740\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.3981 - accuracy: 0.9766\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.3848 - accuracy: 0.9714\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3726 - accuracy: 0.9818\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.3607 - accuracy: 0.9766\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3472 - accuracy: 0.9818\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3347 - accuracy: 0.9818\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.3219 - accuracy: 0.9818\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.3151 - accuracy: 0.9818\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3019 - accuracy: 0.9844\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.2899 - accuracy: 0.9844\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2789 - accuracy: 0.9896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raAPR9Qp7kn1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "89f586ce-c337-4ad3-fb1b-d4a2dcc1d155"
      },
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the mapping\n",
        "dump(mapping, open('mapping.pkl', 'wb'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv4Q4Rkf7kn4"
      },
      "source": [
        "# Generating Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EigunCf57kn6"
      },
      "source": [
        "We must provide sequences of 10 characters as input to the model in order to start the generation process. We will pick these manually. A given input sequence will need to be prepared in the same way as preparing the training data for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UCWctow7kn-"
      },
      "source": [
        "from pickle import load\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # one hot encode\n",
        "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "        # predict character\n",
        "        yhat = np.argmax(model.predict(encoded), axis=-1)\n",
        "        # reverse map integer to character\n",
        "        out_char = ''\n",
        "        for char, index in mapping.items():\n",
        "            if index == yhat:\n",
        "                out_char = char\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += char\n",
        "    return in_text\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "# load the mapping\n",
        "mapping = load(open('mapping.pkl', 'rb'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXn18EY57koA"
      },
      "source": [
        "Running the example generates three sequences of text.\n",
        "\n",
        "The first is a test to see how the model does at starting from the beginning of the rhyme. The second is a test to see how well it does at beginning in the middle of a line. The final example is a test to see how well it does with a sequence of characters never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UhgrJYh7koB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e87db080-5d51-4ab1-a3ea-d1da162c7ca4"
      },
      "source": [
        "# test start of rhyme\n",
        "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
        "# test mid-line\n",
        "print(generate_seq(model, mapping, 10, 'king was i', 20))\n",
        "# test not in original\n",
        "print(generate_seq(model, mapping, 10, 'hello worl', 20))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 460ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Sing a song of sixpence,A pock\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "king was in his counting house\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "hello worls.Four nttth ctblba \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXhN-xhG7koE"
      },
      "source": [
        "If the results aren't satisfactory, try out the suggestions above or these below:\n",
        "- Padding. Update the example to provides sequences line by line only and use padding to fill out each sequence to the maximum line length.\n",
        "- Sequence Length. Experiment with different sequence lengths and see how they impact the behavior of the model.\n",
        "- Tune Model. Experiment with different model configurations, such as the number of memory cells and epochs, and try to develop a better model for fewer resources.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kyXdm-f7koF"
      },
      "source": [
        "# Deliverables to receive credit\n",
        "\n",
        "1. (4 points) Optimize the cells above to tune the model so that it generates text that closely resembles the orginal line from the rhyme, or at least generates sensible words. It's okay if the third example using unseen text still looks somewhat strange  though. Again, this is a toy problem, as language models require a lot of computation. This toy problem is great for rapid experimentation to explore different aspects of deep learning language models.\n",
        "2. (3 points) Write a function to split the text corpus file into training and validation and pipe the validation data into the model.fit() function to be able to track validation error per epoch. Lookup Keras documentation to see how this is handled.\n",
        "3. (3 points) Write a summary (methods and results) in the cells below of the different things you applied. You must include your intuitions behind what did work and what did not work well.\n",
        "4. (Extra Credit 2.5 points) Do something even more interesting. Try a different source text. Train a word-level model. We'll leave it up to your creativity to explore and write a summary of your methods and results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njvIURzq7koG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "054d7ad2-68eb-4425-a1a5-6e4d3ddef3fc"
      },
      "source": [
        "#1 Optimize the Cell\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "\n",
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=[checkpoint])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.6319 - accuracy: 0.0573\n",
            "Epoch 1: loss improved from inf to 3.63193, saving model to best_model.h5\n",
            "3/3 [==============================] - 6s 103ms/step - loss: 3.6319 - accuracy: 0.0573\n",
            "Epoch 2/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.6075 - accuracy: 0.1380\n",
            "Epoch 2: loss improved from 3.63193 to 3.60749, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 3.6075 - accuracy: 0.1380\n",
            "Epoch 3/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.5707 - accuracy: 0.1562\n",
            "Epoch 3: loss improved from 3.60749 to 3.57071, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 3.5707 - accuracy: 0.1562\n",
            "Epoch 4/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.5209 - accuracy: 0.1484\n",
            "Epoch 4: loss improved from 3.57071 to 3.49830, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 3.4983 - accuracy: 0.1589\n",
            "Epoch 5/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.4035 - accuracy: 0.1484\n",
            "Epoch 5: loss improved from 3.49830 to 3.36269, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 3.3627 - accuracy: 0.1562\n",
            "Epoch 6/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1951 - accuracy: 0.1562\n",
            "Epoch 6: loss improved from 3.36269 to 3.19515, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 3.1951 - accuracy: 0.1562\n",
            "Epoch 7/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1826 - accuracy: 0.1589\n",
            "Epoch 7: loss improved from 3.19515 to 3.18263, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 3.1826 - accuracy: 0.1589\n",
            "Epoch 8/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1615 - accuracy: 0.1224\n",
            "Epoch 8: loss improved from 3.18263 to 3.16150, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 3.1615 - accuracy: 0.1224\n",
            "Epoch 9/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1073 - accuracy: 0.1484\n",
            "Epoch 9: loss improved from 3.16150 to 3.10730, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 3.1073 - accuracy: 0.1484\n",
            "Epoch 10/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1106 - accuracy: 0.1406\n",
            "Epoch 10: loss did not improve from 3.10730\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 3.1106 - accuracy: 0.1406\n",
            "Epoch 11/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1117 - accuracy: 0.1510\n",
            "Epoch 11: loss did not improve from 3.10730\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 3.1117 - accuracy: 0.1510\n",
            "Epoch 12/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.1102 - accuracy: 0.1615\n",
            "Epoch 12: loss did not improve from 3.10730\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 3.1102 - accuracy: 0.1615\n",
            "Epoch 13/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0756 - accuracy: 0.1589\n",
            "Epoch 13: loss improved from 3.10730 to 3.07562, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 3.0756 - accuracy: 0.1589\n",
            "Epoch 14/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0900 - accuracy: 0.1536\n",
            "Epoch 14: loss did not improve from 3.07562\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 3.0900 - accuracy: 0.1536\n",
            "Epoch 15/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0777 - accuracy: 0.1589\n",
            "Epoch 15: loss did not improve from 3.07562\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 3.0777 - accuracy: 0.1589\n",
            "Epoch 16/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0626 - accuracy: 0.1589\n",
            "Epoch 16: loss improved from 3.07562 to 3.06258, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 3.0626 - accuracy: 0.1589\n",
            "Epoch 17/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0714 - accuracy: 0.1589\n",
            "Epoch 17: loss did not improve from 3.06258\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 3.0714 - accuracy: 0.1589\n",
            "Epoch 18/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0451 - accuracy: 0.1667\n",
            "Epoch 18: loss improved from 3.06258 to 3.04510, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 3.0451 - accuracy: 0.1667\n",
            "Epoch 19/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0584 - accuracy: 0.1536\n",
            "Epoch 19: loss did not improve from 3.04510\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 3.0584 - accuracy: 0.1536\n",
            "Epoch 20/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0453 - accuracy: 0.1615\n",
            "Epoch 20: loss did not improve from 3.04510\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 3.0453 - accuracy: 0.1615\n",
            "Epoch 21/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0317 - accuracy: 0.1641\n",
            "Epoch 21: loss improved from 3.04510 to 3.03174, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 3.0317 - accuracy: 0.1641\n",
            "Epoch 22/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0349 - accuracy: 0.1745\n",
            "Epoch 22: loss did not improve from 3.03174\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 3.0349 - accuracy: 0.1745\n",
            "Epoch 23/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.9917 - accuracy: 0.1641\n",
            "Epoch 23: loss improved from 3.03174 to 2.99167, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 2.9917 - accuracy: 0.1641\n",
            "Epoch 24/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0124 - accuracy: 0.1745\n",
            "Epoch 24: loss did not improve from 2.99167\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 3.0124 - accuracy: 0.1745\n",
            "Epoch 25/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 3.0118 - accuracy: 0.1797\n",
            "Epoch 25: loss did not improve from 2.99167\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 3.0118 - accuracy: 0.1797\n",
            "Epoch 26/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.9775 - accuracy: 0.1693\n",
            "Epoch 26: loss improved from 2.99167 to 2.97747, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 2.9775 - accuracy: 0.1693\n",
            "Epoch 27/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.9843 - accuracy: 0.1719\n",
            "Epoch 27: loss did not improve from 2.97747\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 2.9843 - accuracy: 0.1719\n",
            "Epoch 28/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.9693 - accuracy: 0.1927\n",
            "Epoch 28: loss improved from 2.97747 to 2.96929, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 2.9693 - accuracy: 0.1927\n",
            "Epoch 29/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 3.0126 - accuracy: 0.1602\n",
            "Epoch 29: loss improved from 2.96929 to 2.94763, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 2.9476 - accuracy: 0.1745\n",
            "Epoch 30/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.9355 - accuracy: 0.1849\n",
            "Epoch 30: loss improved from 2.94763 to 2.93546, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 2.9355 - accuracy: 0.1849\n",
            "Epoch 31/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.9262 - accuracy: 0.1875\n",
            "Epoch 31: loss improved from 2.93546 to 2.92616, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.9262 - accuracy: 0.1875\n",
            "Epoch 32/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8965 - accuracy: 0.1875\n",
            "Epoch 32: loss improved from 2.92616 to 2.89648, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 2.8965 - accuracy: 0.1875\n",
            "Epoch 33/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8900 - accuracy: 0.1719\n",
            "Epoch 33: loss improved from 2.89648 to 2.88997, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 2.8900 - accuracy: 0.1719\n",
            "Epoch 34/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8591 - accuracy: 0.1901\n",
            "Epoch 34: loss improved from 2.88997 to 2.85908, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 2.8591 - accuracy: 0.1901\n",
            "Epoch 35/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.9119 - accuracy: 0.1523\n",
            "Epoch 35: loss improved from 2.85908 to 2.84879, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 2.8488 - accuracy: 0.1719\n",
            "Epoch 36/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8288 - accuracy: 0.1875\n",
            "Epoch 36: loss improved from 2.84879 to 2.82884, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.8288 - accuracy: 0.1875\n",
            "Epoch 37/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.8204 - accuracy: 0.1771\n",
            "Epoch 37: loss improved from 2.82884 to 2.82041, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 2.8204 - accuracy: 0.1771\n",
            "Epoch 38/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.7736 - accuracy: 0.2083\n",
            "Epoch 38: loss improved from 2.82041 to 2.77357, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 2.7736 - accuracy: 0.2083\n",
            "Epoch 39/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.7672 - accuracy: 0.1979\n",
            "Epoch 39: loss improved from 2.77357 to 2.76723, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 2.7672 - accuracy: 0.1979\n",
            "Epoch 40/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.7634 - accuracy: 0.2292\n",
            "Epoch 40: loss improved from 2.76723 to 2.76336, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 2.7634 - accuracy: 0.2292\n",
            "Epoch 41/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 2.7130 - accuracy: 0.2422\n",
            "Epoch 41: loss improved from 2.76336 to 2.71455, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 2.7146 - accuracy: 0.2292\n",
            "Epoch 42/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.7257 - accuracy: 0.2135\n",
            "Epoch 42: loss did not improve from 2.71455\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 2.7257 - accuracy: 0.2135\n",
            "Epoch 43/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.7094 - accuracy: 0.2083\n",
            "Epoch 43: loss improved from 2.71455 to 2.70939, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 2.7094 - accuracy: 0.2083\n",
            "Epoch 44/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6694 - accuracy: 0.2240\n",
            "Epoch 44: loss improved from 2.70939 to 2.66936, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 2.6694 - accuracy: 0.2240\n",
            "Epoch 45/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6453 - accuracy: 0.1849\n",
            "Epoch 45: loss improved from 2.66936 to 2.64533, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 2.6453 - accuracy: 0.1849\n",
            "Epoch 46/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6053 - accuracy: 0.2500\n",
            "Epoch 46: loss improved from 2.64533 to 2.60526, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 2.6053 - accuracy: 0.2500\n",
            "Epoch 47/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.6156 - accuracy: 0.2292\n",
            "Epoch 47: loss did not improve from 2.60526\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 2.6156 - accuracy: 0.2292\n",
            "Epoch 48/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5782 - accuracy: 0.2214\n",
            "Epoch 48: loss improved from 2.60526 to 2.57820, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 2.5782 - accuracy: 0.2214\n",
            "Epoch 49/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5602 - accuracy: 0.2240\n",
            "Epoch 49: loss improved from 2.57820 to 2.56021, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 2.5602 - accuracy: 0.2240\n",
            "Epoch 50/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.5511 - accuracy: 0.2240\n",
            "Epoch 50: loss improved from 2.56021 to 2.55113, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 2.5511 - accuracy: 0.2240\n",
            "Epoch 51/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4904 - accuracy: 0.2396\n",
            "Epoch 51: loss improved from 2.55113 to 2.49040, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 2.4904 - accuracy: 0.2396\n",
            "Epoch 52/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4384 - accuracy: 0.2604\n",
            "Epoch 52: loss improved from 2.49040 to 2.43838, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 2.4384 - accuracy: 0.2604\n",
            "Epoch 53/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4509 - accuracy: 0.2630\n",
            "Epoch 53: loss did not improve from 2.43838\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 2.4509 - accuracy: 0.2630\n",
            "Epoch 54/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4075 - accuracy: 0.2526\n",
            "Epoch 54: loss improved from 2.43838 to 2.40752, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.4075 - accuracy: 0.2526\n",
            "Epoch 55/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.4197 - accuracy: 0.2578\n",
            "Epoch 55: loss did not improve from 2.40752\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 2.4197 - accuracy: 0.2578\n",
            "Epoch 56/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3519 - accuracy: 0.2682\n",
            "Epoch 56: loss improved from 2.40752 to 2.35187, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 2.3519 - accuracy: 0.2682\n",
            "Epoch 57/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3406 - accuracy: 0.2604\n",
            "Epoch 57: loss improved from 2.35187 to 2.34064, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 2.3406 - accuracy: 0.2604\n",
            "Epoch 58/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3397 - accuracy: 0.2839\n",
            "Epoch 58: loss improved from 2.34064 to 2.33974, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 2.3397 - accuracy: 0.2839\n",
            "Epoch 59/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.3049 - accuracy: 0.2760\n",
            "Epoch 59: loss improved from 2.33974 to 2.30493, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 2.3049 - accuracy: 0.2760\n",
            "Epoch 60/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.2616 - accuracy: 0.2552\n",
            "Epoch 60: loss improved from 2.30493 to 2.26160, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 2.2616 - accuracy: 0.2552\n",
            "Epoch 61/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.2192 - accuracy: 0.2995\n",
            "Epoch 61: loss improved from 2.26160 to 2.21922, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 2.2192 - accuracy: 0.2995\n",
            "Epoch 62/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.2090 - accuracy: 0.3099\n",
            "Epoch 62: loss improved from 2.21922 to 2.20899, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 2.2090 - accuracy: 0.3099\n",
            "Epoch 63/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.1272 - accuracy: 0.3255\n",
            "Epoch 63: loss improved from 2.20899 to 2.12716, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 2.1272 - accuracy: 0.3255\n",
            "Epoch 64/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.1226 - accuracy: 0.3307\n",
            "Epoch 64: loss improved from 2.12716 to 2.12261, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 2.1226 - accuracy: 0.3307\n",
            "Epoch 65/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.0943 - accuracy: 0.3438\n",
            "Epoch 65: loss improved from 2.12261 to 2.09434, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 2.0943 - accuracy: 0.3438\n",
            "Epoch 66/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.0531 - accuracy: 0.3516\n",
            "Epoch 66: loss improved from 2.09434 to 2.05312, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 2.0531 - accuracy: 0.3516\n",
            "Epoch 67/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 2.0718 - accuracy: 0.3516\n",
            "Epoch 67: loss did not improve from 2.05312\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 2.0718 - accuracy: 0.3516\n",
            "Epoch 68/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9982 - accuracy: 0.4010\n",
            "Epoch 68: loss improved from 2.05312 to 1.99821, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 1.9982 - accuracy: 0.4010\n",
            "Epoch 69/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9990 - accuracy: 0.3984\n",
            "Epoch 69: loss did not improve from 1.99821\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 1.9990 - accuracy: 0.3984\n",
            "Epoch 70/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9637 - accuracy: 0.4141\n",
            "Epoch 70: loss improved from 1.99821 to 1.96374, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 1.9637 - accuracy: 0.4141\n",
            "Epoch 71/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9389 - accuracy: 0.3698\n",
            "Epoch 71: loss improved from 1.96374 to 1.93894, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 1.9389 - accuracy: 0.3698\n",
            "Epoch 72/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.9014 - accuracy: 0.3932\n",
            "Epoch 72: loss improved from 1.93894 to 1.90139, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 1.9014 - accuracy: 0.3932\n",
            "Epoch 73/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8654 - accuracy: 0.4089\n",
            "Epoch 73: loss improved from 1.90139 to 1.86537, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 1.8654 - accuracy: 0.4089\n",
            "Epoch 74/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8314 - accuracy: 0.4193\n",
            "Epoch 74: loss improved from 1.86537 to 1.83142, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 1.8314 - accuracy: 0.4193\n",
            "Epoch 75/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.7985 - accuracy: 0.4505\n",
            "Epoch 75: loss improved from 1.83142 to 1.79855, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 1.7985 - accuracy: 0.4505\n",
            "Epoch 76/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.8265 - accuracy: 0.4557\n",
            "Epoch 76: loss did not improve from 1.79855\n",
            "3/3 [==============================] - 0s 89ms/step - loss: 1.8265 - accuracy: 0.4557\n",
            "Epoch 77/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.7477 - accuracy: 0.4740\n",
            "Epoch 77: loss improved from 1.79855 to 1.74774, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 1.7477 - accuracy: 0.4740\n",
            "Epoch 78/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.7184 - accuracy: 0.4792\n",
            "Epoch 78: loss improved from 1.74774 to 1.71841, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 119ms/step - loss: 1.7184 - accuracy: 0.4792\n",
            "Epoch 79/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6657 - accuracy: 0.4948\n",
            "Epoch 79: loss improved from 1.71841 to 1.66574, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 1.6657 - accuracy: 0.4948\n",
            "Epoch 80/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6874 - accuracy: 0.5052\n",
            "Epoch 80: loss did not improve from 1.66574\n",
            "3/3 [==============================] - 0s 97ms/step - loss: 1.6874 - accuracy: 0.5052\n",
            "Epoch 81/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.6246 - accuracy: 0.5208\n",
            "Epoch 81: loss improved from 1.66574 to 1.62464, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 1.6246 - accuracy: 0.5208\n",
            "Epoch 82/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.5908 - accuracy: 0.5156\n",
            "Epoch 82: loss improved from 1.62464 to 1.59078, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 1.5908 - accuracy: 0.5156\n",
            "Epoch 83/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.5469 - accuracy: 0.5312\n",
            "Epoch 83: loss improved from 1.59078 to 1.54690, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 118ms/step - loss: 1.5469 - accuracy: 0.5312\n",
            "Epoch 84/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.5023 - accuracy: 0.5417\n",
            "Epoch 84: loss improved from 1.54690 to 1.50234, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 1.5023 - accuracy: 0.5417\n",
            "Epoch 85/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4906 - accuracy: 0.5286\n",
            "Epoch 85: loss improved from 1.50234 to 1.49062, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 99ms/step - loss: 1.4906 - accuracy: 0.5286\n",
            "Epoch 86/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4874 - accuracy: 0.5729\n",
            "Epoch 86: loss improved from 1.49062 to 1.48740, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 1.4874 - accuracy: 0.5729\n",
            "Epoch 87/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4513 - accuracy: 0.5807\n",
            "Epoch 87: loss improved from 1.48740 to 1.45135, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 1.4513 - accuracy: 0.5807\n",
            "Epoch 88/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4628 - accuracy: 0.5651\n",
            "Epoch 88: loss did not improve from 1.45135\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 1.4628 - accuracy: 0.5651\n",
            "Epoch 89/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4620 - accuracy: 0.5781\n",
            "Epoch 89: loss did not improve from 1.45135\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 1.4620 - accuracy: 0.5781\n",
            "Epoch 90/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.4327 - accuracy: 0.5521\n",
            "Epoch 90: loss improved from 1.45135 to 1.43265, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 1.4327 - accuracy: 0.5521\n",
            "Epoch 91/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3900 - accuracy: 0.5755\n",
            "Epoch 91: loss improved from 1.43265 to 1.38996, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 1.3900 - accuracy: 0.5755\n",
            "Epoch 92/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3128 - accuracy: 0.6250\n",
            "Epoch 92: loss improved from 1.38996 to 1.31283, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 1.3128 - accuracy: 0.6250\n",
            "Epoch 93/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.3549 - accuracy: 0.5911\n",
            "Epoch 93: loss did not improve from 1.31283\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 1.3549 - accuracy: 0.5911\n",
            "Epoch 94/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2669 - accuracy: 0.6094\n",
            "Epoch 94: loss improved from 1.31283 to 1.29825, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 1.2983 - accuracy: 0.6016\n",
            "Epoch 95/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2824 - accuracy: 0.6250\n",
            "Epoch 95: loss improved from 1.29825 to 1.28243, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 1.2824 - accuracy: 0.6250\n",
            "Epoch 96/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2557 - accuracy: 0.6510\n",
            "Epoch 96: loss improved from 1.28243 to 1.25569, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 1.2557 - accuracy: 0.6510\n",
            "Epoch 97/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.2122 - accuracy: 0.6523\n",
            "Epoch 97: loss improved from 1.25569 to 1.21715, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 1.2172 - accuracy: 0.6406\n",
            "Epoch 98/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.2113 - accuracy: 0.6667\n",
            "Epoch 98: loss improved from 1.21715 to 1.21129, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 1.2113 - accuracy: 0.6667\n",
            "Epoch 99/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1696 - accuracy: 0.6667\n",
            "Epoch 99: loss improved from 1.21129 to 1.16958, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 1.1696 - accuracy: 0.6667\n",
            "Epoch 100/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1869 - accuracy: 0.6693\n",
            "Epoch 100: loss did not improve from 1.16958\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 1.1869 - accuracy: 0.6693\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c16adfdb670>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh9kQZnr7koK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "53dac8b8-0965-4146-8337-b20bd816adf3"
      },
      "source": [
        "#2 Split the Text\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val), callbacks=[checkpoint])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.1393 - accuracy: 0.6725\n",
            "Epoch 1: loss improved from 1.16958 to 1.13933, saving model to best_model.h5\n",
            "3/3 [==============================] - 4s 502ms/step - loss: 1.1393 - accuracy: 0.6725 - val_loss: 0.9573 - val_accuracy: 0.6923\n",
            "Epoch 2/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 1.1696 - accuracy: 0.6602\n",
            "Epoch 2: loss improved from 1.13933 to 1.13837, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 1.1384 - accuracy: 0.6667 - val_loss: 0.9964 - val_accuracy: 0.6923\n",
            "Epoch 3/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0565 - accuracy: 0.6957\n",
            "Epoch 3: loss improved from 1.13837 to 1.05647, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 1.0565 - accuracy: 0.6957 - val_loss: 1.0581 - val_accuracy: 0.6923\n",
            "Epoch 4/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0396 - accuracy: 0.7101\n",
            "Epoch 4: loss improved from 1.05647 to 1.03961, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 70ms/step - loss: 1.0396 - accuracy: 0.7101 - val_loss: 1.0844 - val_accuracy: 0.6923\n",
            "Epoch 5/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0557 - accuracy: 0.7043\n",
            "Epoch 5: loss did not improve from 1.03961\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 1.0557 - accuracy: 0.7043 - val_loss: 1.1580 - val_accuracy: 0.6154\n",
            "Epoch 6/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0018 - accuracy: 0.7362\n",
            "Epoch 6: loss improved from 1.03961 to 1.00182, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 1.0018 - accuracy: 0.7362 - val_loss: 1.1680 - val_accuracy: 0.6154\n",
            "Epoch 7/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9920 - accuracy: 0.7333\n",
            "Epoch 7: loss improved from 1.00182 to 0.99204, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 0.9920 - accuracy: 0.7333 - val_loss: 1.1770 - val_accuracy: 0.5897\n",
            "Epoch 8/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 1.0118 - accuracy: 0.7246\n",
            "Epoch 8: loss did not improve from 0.99204\n",
            "3/3 [==============================] - 0s 129ms/step - loss: 1.0118 - accuracy: 0.7246 - val_loss: 1.1950 - val_accuracy: 0.6154\n",
            "Epoch 9/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9667 - accuracy: 0.7478\n",
            "Epoch 9: loss improved from 0.99204 to 0.96673, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.9667 - accuracy: 0.7478 - val_loss: 1.1877 - val_accuracy: 0.6667\n",
            "Epoch 10/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9674 - accuracy: 0.7362\n",
            "Epoch 10: loss did not improve from 0.96673\n",
            "3/3 [==============================] - 0s 112ms/step - loss: 0.9674 - accuracy: 0.7362 - val_loss: 1.3311 - val_accuracy: 0.5897\n",
            "Epoch 11/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9392 - accuracy: 0.7246\n",
            "Epoch 11: loss improved from 0.96673 to 0.93922, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 130ms/step - loss: 0.9392 - accuracy: 0.7246 - val_loss: 1.3394 - val_accuracy: 0.5641\n",
            "Epoch 12/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.9339 - accuracy: 0.7362\n",
            "Epoch 12: loss improved from 0.93922 to 0.93391, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.9339 - accuracy: 0.7362 - val_loss: 1.3392 - val_accuracy: 0.5385\n",
            "Epoch 13/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8664 - accuracy: 0.7797\n",
            "Epoch 13: loss improved from 0.93391 to 0.86640, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 120ms/step - loss: 0.8664 - accuracy: 0.7797 - val_loss: 1.4036 - val_accuracy: 0.5897\n",
            "Epoch 14/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8927 - accuracy: 0.7797\n",
            "Epoch 14: loss did not improve from 0.86640\n",
            "3/3 [==============================] - 0s 116ms/step - loss: 0.8927 - accuracy: 0.7797 - val_loss: 1.3681 - val_accuracy: 0.5128\n",
            "Epoch 15/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8752 - accuracy: 0.7652\n",
            "Epoch 15: loss did not improve from 0.86640\n",
            "3/3 [==============================] - 0s 100ms/step - loss: 0.8752 - accuracy: 0.7652 - val_loss: 1.4172 - val_accuracy: 0.5641\n",
            "Epoch 16/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8586 - accuracy: 0.7681\n",
            "Epoch 16: loss improved from 0.86640 to 0.85864, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 125ms/step - loss: 0.8586 - accuracy: 0.7681 - val_loss: 1.5122 - val_accuracy: 0.5385\n",
            "Epoch 17/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8281 - accuracy: 0.8029\n",
            "Epoch 17: loss improved from 0.85864 to 0.82814, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 127ms/step - loss: 0.8281 - accuracy: 0.8029 - val_loss: 1.4151 - val_accuracy: 0.5385\n",
            "Epoch 18/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8348 - accuracy: 0.7681\n",
            "Epoch 18: loss did not improve from 0.82814\n",
            "3/3 [==============================] - 0s 179ms/step - loss: 0.8348 - accuracy: 0.7681 - val_loss: 1.5216 - val_accuracy: 0.4615\n",
            "Epoch 19/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.8082 - accuracy: 0.7884\n",
            "Epoch 19: loss improved from 0.82814 to 0.80821, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 179ms/step - loss: 0.8082 - accuracy: 0.7884 - val_loss: 1.5550 - val_accuracy: 0.4615\n",
            "Epoch 20/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.8348\n",
            "Epoch 20: loss improved from 0.80821 to 0.74188, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 177ms/step - loss: 0.7419 - accuracy: 0.8348 - val_loss: 1.5499 - val_accuracy: 0.4615\n",
            "Epoch 21/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.8290\n",
            "Epoch 21: loss did not improve from 0.74188\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 0.7501 - accuracy: 0.8290 - val_loss: 1.5506 - val_accuracy: 0.5128\n",
            "Epoch 22/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7484 - accuracy: 0.8000\n",
            "Epoch 22: loss did not improve from 0.74188\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.7484 - accuracy: 0.8000 - val_loss: 1.5404 - val_accuracy: 0.4872\n",
            "Epoch 23/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7532 - accuracy: 0.8319\n",
            "Epoch 23: loss did not improve from 0.74188\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.7532 - accuracy: 0.8319 - val_loss: 1.5750 - val_accuracy: 0.5128\n",
            "Epoch 24/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.7091 - accuracy: 0.8319\n",
            "Epoch 24: loss improved from 0.74188 to 0.70906, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.7091 - accuracy: 0.8319 - val_loss: 1.5362 - val_accuracy: 0.4872\n",
            "Epoch 25/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6856 - accuracy: 0.8145\n",
            "Epoch 25: loss improved from 0.70906 to 0.68561, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.6856 - accuracy: 0.8145 - val_loss: 1.5790 - val_accuracy: 0.4872\n",
            "Epoch 26/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.6737 - accuracy: 0.8398\n",
            "Epoch 26: loss improved from 0.68561 to 0.67655, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.6766 - accuracy: 0.8464 - val_loss: 1.6367 - val_accuracy: 0.4615\n",
            "Epoch 27/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.6721 - accuracy: 0.8281\n",
            "Epoch 27: loss improved from 0.67655 to 0.66414, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.6641 - accuracy: 0.8464 - val_loss: 1.6683 - val_accuracy: 0.5128\n",
            "Epoch 28/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.6855 - accuracy: 0.8516\n",
            "Epoch 28: loss did not improve from 0.66414\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.6721 - accuracy: 0.8493 - val_loss: 1.6926 - val_accuracy: 0.4615\n",
            "Epoch 29/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6066 - accuracy: 0.8696\n",
            "Epoch 29: loss improved from 0.66414 to 0.60665, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.6066 - accuracy: 0.8696 - val_loss: 1.7190 - val_accuracy: 0.4615\n",
            "Epoch 30/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.6370 - accuracy: 0.8438\n",
            "Epoch 30: loss did not improve from 0.60665\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.6435 - accuracy: 0.8464 - val_loss: 1.7653 - val_accuracy: 0.4615\n",
            "Epoch 31/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.6336 - accuracy: 0.8551\n",
            "Epoch 31: loss did not improve from 0.60665\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.6336 - accuracy: 0.8551 - val_loss: 1.7581 - val_accuracy: 0.4872\n",
            "Epoch 32/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5989 - accuracy: 0.8754\n",
            "Epoch 32: loss improved from 0.60665 to 0.59886, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.5989 - accuracy: 0.8754 - val_loss: 1.7624 - val_accuracy: 0.4615\n",
            "Epoch 33/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5622 - accuracy: 0.8984\n",
            "Epoch 33: loss improved from 0.59886 to 0.57635, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.5764 - accuracy: 0.8783 - val_loss: 1.8706 - val_accuracy: 0.4615\n",
            "Epoch 34/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5615 - accuracy: 0.8638\n",
            "Epoch 34: loss improved from 0.57635 to 0.56148, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.5615 - accuracy: 0.8638 - val_loss: 1.8088 - val_accuracy: 0.5128\n",
            "Epoch 35/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.8957\n",
            "Epoch 35: loss improved from 0.56148 to 0.55627, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5563 - accuracy: 0.8957 - val_loss: 1.7553 - val_accuracy: 0.4615\n",
            "Epoch 36/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5527 - accuracy: 0.8754\n",
            "Epoch 36: loss improved from 0.55627 to 0.55275, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.5527 - accuracy: 0.8754 - val_loss: 1.8350 - val_accuracy: 0.4615\n",
            "Epoch 37/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5631 - accuracy: 0.8750\n",
            "Epoch 37: loss did not improve from 0.55275\n",
            "3/3 [==============================] - 0s 95ms/step - loss: 0.5564 - accuracy: 0.8696 - val_loss: 1.7722 - val_accuracy: 0.4615\n",
            "Epoch 38/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5476 - accuracy: 0.8633\n",
            "Epoch 38: loss improved from 0.55275 to 0.54753, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.5475 - accuracy: 0.8638 - val_loss: 1.9079 - val_accuracy: 0.4103\n",
            "Epoch 39/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5210 - accuracy: 0.8984\n",
            "Epoch 39: loss improved from 0.54753 to 0.53750, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 91ms/step - loss: 0.5375 - accuracy: 0.8812 - val_loss: 1.9215 - val_accuracy: 0.4103\n",
            "Epoch 40/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5159 - accuracy: 0.8867\n",
            "Epoch 40: loss improved from 0.53750 to 0.51548, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.5155 - accuracy: 0.8754 - val_loss: 1.8336 - val_accuracy: 0.4872\n",
            "Epoch 41/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4909 - accuracy: 0.8945\n",
            "Epoch 41: loss improved from 0.51548 to 0.51078, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 0.5108 - accuracy: 0.8899 - val_loss: 1.8657 - val_accuracy: 0.4615\n",
            "Epoch 42/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.5259 - accuracy: 0.8711\n",
            "Epoch 42: loss did not improve from 0.51078\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.5151 - accuracy: 0.8812 - val_loss: 1.9430 - val_accuracy: 0.4103\n",
            "Epoch 43/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4480 - accuracy: 0.9453\n",
            "Epoch 43: loss improved from 0.51078 to 0.48864, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 88ms/step - loss: 0.4886 - accuracy: 0.9188 - val_loss: 1.9263 - val_accuracy: 0.4615\n",
            "Epoch 44/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4895 - accuracy: 0.8783\n",
            "Epoch 44: loss did not improve from 0.48864\n",
            "3/3 [==============================] - 0s 76ms/step - loss: 0.4895 - accuracy: 0.8783 - val_loss: 1.9623 - val_accuracy: 0.3590\n",
            "Epoch 45/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4909 - accuracy: 0.8945\n",
            "Epoch 45: loss did not improve from 0.48864\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.4892 - accuracy: 0.9014 - val_loss: 1.9126 - val_accuracy: 0.4615\n",
            "Epoch 46/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.9246\n",
            "Epoch 46: loss improved from 0.48864 to 0.45231, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 98ms/step - loss: 0.4523 - accuracy: 0.9246 - val_loss: 1.8659 - val_accuracy: 0.3590\n",
            "Epoch 47/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4625 - accuracy: 0.8984\n",
            "Epoch 47: loss did not improve from 0.45231\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.4588 - accuracy: 0.9072 - val_loss: 1.9099 - val_accuracy: 0.3846\n",
            "Epoch 48/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.9130\n",
            "Epoch 48: loss improved from 0.45231 to 0.44782, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 104ms/step - loss: 0.4478 - accuracy: 0.9130 - val_loss: 1.9508 - val_accuracy: 0.4615\n",
            "Epoch 49/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.9072\n",
            "Epoch 49: loss improved from 0.44782 to 0.44776, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 93ms/step - loss: 0.4478 - accuracy: 0.9072 - val_loss: 1.9192 - val_accuracy: 0.4359\n",
            "Epoch 50/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.4351 - accuracy: 0.9258\n",
            "Epoch 50: loss improved from 0.44776 to 0.43457, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 83ms/step - loss: 0.4346 - accuracy: 0.9246 - val_loss: 2.0352 - val_accuracy: 0.4103\n",
            "Epoch 51/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4278 - accuracy: 0.9101\n",
            "Epoch 51: loss improved from 0.43457 to 0.42783, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 0.4278 - accuracy: 0.9101 - val_loss: 2.0172 - val_accuracy: 0.4103\n",
            "Epoch 52/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4038 - accuracy: 0.9333\n",
            "Epoch 52: loss improved from 0.42783 to 0.40378, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 128ms/step - loss: 0.4038 - accuracy: 0.9333 - val_loss: 2.0020 - val_accuracy: 0.3590\n",
            "Epoch 53/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3737 - accuracy: 0.9297\n",
            "Epoch 53: loss improved from 0.40378 to 0.37958, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 0.3796 - accuracy: 0.9333 - val_loss: 2.0792 - val_accuracy: 0.4103\n",
            "Epoch 54/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.9072\n",
            "Epoch 54: loss did not improve from 0.37958\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.4185 - accuracy: 0.9072 - val_loss: 2.0055 - val_accuracy: 0.3846\n",
            "Epoch 55/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.9130\n",
            "Epoch 55: loss did not improve from 0.37958\n",
            "3/3 [==============================] - 0s 122ms/step - loss: 0.3997 - accuracy: 0.9130 - val_loss: 2.0288 - val_accuracy: 0.3590\n",
            "Epoch 56/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4167 - accuracy: 0.9072\n",
            "Epoch 56: loss did not improve from 0.37958\n",
            "3/3 [==============================] - 0s 92ms/step - loss: 0.4167 - accuracy: 0.9072 - val_loss: 2.1268 - val_accuracy: 0.3846\n",
            "Epoch 57/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.4106 - accuracy: 0.9159\n",
            "Epoch 57: loss did not improve from 0.37958\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.4106 - accuracy: 0.9159 - val_loss: 2.0741 - val_accuracy: 0.3590\n",
            "Epoch 58/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3980 - accuracy: 0.9180\n",
            "Epoch 58: loss did not improve from 0.37958\n",
            "3/3 [==============================] - 0s 68ms/step - loss: 0.3942 - accuracy: 0.9217 - val_loss: 2.0417 - val_accuracy: 0.4103\n",
            "Epoch 59/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3570 - accuracy: 0.9565\n",
            "Epoch 59: loss improved from 0.37958 to 0.35699, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.3570 - accuracy: 0.9565 - val_loss: 2.0445 - val_accuracy: 0.3846\n",
            "Epoch 60/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3785 - accuracy: 0.9336\n",
            "Epoch 60: loss did not improve from 0.35699\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.3775 - accuracy: 0.9333 - val_loss: 2.1426 - val_accuracy: 0.3846\n",
            "Epoch 61/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3571 - accuracy: 0.9453\n",
            "Epoch 61: loss did not improve from 0.35699\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.3645 - accuracy: 0.9420 - val_loss: 2.1072 - val_accuracy: 0.3846\n",
            "Epoch 62/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3339 - accuracy: 0.9336\n",
            "Epoch 62: loss improved from 0.35699 to 0.34250, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3425 - accuracy: 0.9304 - val_loss: 2.0548 - val_accuracy: 0.4359\n",
            "Epoch 63/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3189 - accuracy: 0.9594\n",
            "Epoch 63: loss improved from 0.34250 to 0.31890, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 77ms/step - loss: 0.3189 - accuracy: 0.9594 - val_loss: 2.0567 - val_accuracy: 0.3846\n",
            "Epoch 64/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.3438 - accuracy: 0.9414\n",
            "Epoch 64: loss did not improve from 0.31890\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 0.3378 - accuracy: 0.9449 - val_loss: 2.1338 - val_accuracy: 0.4103\n",
            "Epoch 65/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.9246\n",
            "Epoch 65: loss did not improve from 0.31890\n",
            "3/3 [==============================] - 0s 61ms/step - loss: 0.3484 - accuracy: 0.9246 - val_loss: 2.1090 - val_accuracy: 0.3590\n",
            "Epoch 66/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3227 - accuracy: 0.9507\n",
            "Epoch 66: loss did not improve from 0.31890\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 0.3227 - accuracy: 0.9507 - val_loss: 2.0801 - val_accuracy: 0.3590\n",
            "Epoch 67/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.9304\n",
            "Epoch 67: loss did not improve from 0.31890\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.3441 - accuracy: 0.9304 - val_loss: 2.0725 - val_accuracy: 0.4103\n",
            "Epoch 68/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.9478\n",
            "Epoch 68: loss improved from 0.31890 to 0.30811, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 0.3081 - accuracy: 0.9478 - val_loss: 2.1365 - val_accuracy: 0.3846\n",
            "Epoch 69/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3203 - accuracy: 0.9478\n",
            "Epoch 69: loss did not improve from 0.30811\n",
            "3/3 [==============================] - 0s 130ms/step - loss: 0.3203 - accuracy: 0.9478 - val_loss: 2.1393 - val_accuracy: 0.3590\n",
            "Epoch 70/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.9391\n",
            "Epoch 70: loss did not improve from 0.30811\n",
            "3/3 [==============================] - 0s 106ms/step - loss: 0.3294 - accuracy: 0.9391 - val_loss: 2.1419 - val_accuracy: 0.3590\n",
            "Epoch 71/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3258 - accuracy: 0.9536\n",
            "Epoch 71: loss did not improve from 0.30811\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 0.3258 - accuracy: 0.9536 - val_loss: 2.1857 - val_accuracy: 0.3590\n",
            "Epoch 72/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9681\n",
            "Epoch 72: loss improved from 0.30811 to 0.28500, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 128ms/step - loss: 0.2850 - accuracy: 0.9681 - val_loss: 2.2363 - val_accuracy: 0.3846\n",
            "Epoch 73/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.9478\n",
            "Epoch 73: loss did not improve from 0.28500\n",
            "3/3 [==============================] - 0s 109ms/step - loss: 0.3020 - accuracy: 0.9478 - val_loss: 2.1533 - val_accuracy: 0.3590\n",
            "Epoch 74/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.3193 - accuracy: 0.9478\n",
            "Epoch 74: loss did not improve from 0.28500\n",
            "3/3 [==============================] - 0s 101ms/step - loss: 0.3193 - accuracy: 0.9478 - val_loss: 2.1691 - val_accuracy: 0.3590\n",
            "Epoch 75/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.9478\n",
            "Epoch 75: loss did not improve from 0.28500\n",
            "3/3 [==============================] - 0s 121ms/step - loss: 0.2999 - accuracy: 0.9478 - val_loss: 2.2067 - val_accuracy: 0.3333\n",
            "Epoch 76/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.9536\n",
            "Epoch 76: loss improved from 0.28500 to 0.26781, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 131ms/step - loss: 0.2678 - accuracy: 0.9536 - val_loss: 2.2375 - val_accuracy: 0.3333\n",
            "Epoch 77/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.9565\n",
            "Epoch 77: loss did not improve from 0.26781\n",
            "3/3 [==============================] - 0s 110ms/step - loss: 0.2836 - accuracy: 0.9565 - val_loss: 2.3692 - val_accuracy: 0.3077\n",
            "Epoch 78/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9507\n",
            "Epoch 78: loss improved from 0.26781 to 0.26617, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 124ms/step - loss: 0.2662 - accuracy: 0.9507 - val_loss: 2.3253 - val_accuracy: 0.3333\n",
            "Epoch 79/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.9652\n",
            "Epoch 79: loss did not improve from 0.26617\n",
            "3/3 [==============================] - 0s 103ms/step - loss: 0.2753 - accuracy: 0.9652 - val_loss: 2.2602 - val_accuracy: 0.3333\n",
            "Epoch 80/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2939 - accuracy: 0.9391\n",
            "Epoch 80: loss did not improve from 0.26617\n",
            "3/3 [==============================] - 0s 113ms/step - loss: 0.2939 - accuracy: 0.9391 - val_loss: 2.3240 - val_accuracy: 0.3590\n",
            "Epoch 81/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.9449\n",
            "Epoch 81: loss did not improve from 0.26617\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 0.2714 - accuracy: 0.9449 - val_loss: 2.3337 - val_accuracy: 0.3333\n",
            "Epoch 82/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.9594\n",
            "Epoch 82: loss did not improve from 0.26617\n",
            "3/3 [==============================] - 0s 115ms/step - loss: 0.2760 - accuracy: 0.9594 - val_loss: 2.3216 - val_accuracy: 0.3333\n",
            "Epoch 83/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9565\n",
            "Epoch 83: loss improved from 0.26617 to 0.26099, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.2610 - accuracy: 0.9565 - val_loss: 2.2817 - val_accuracy: 0.3590\n",
            "Epoch 84/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2339 - accuracy: 0.9531\n",
            "Epoch 84: loss improved from 0.26099 to 0.25794, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2579 - accuracy: 0.9478 - val_loss: 2.3002 - val_accuracy: 0.3846\n",
            "Epoch 85/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9594\n",
            "Epoch 85: loss did not improve from 0.25794\n",
            "3/3 [==============================] - 0s 59ms/step - loss: 0.2618 - accuracy: 0.9594 - val_loss: 2.3250 - val_accuracy: 0.3846\n",
            "Epoch 86/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2387 - accuracy: 0.9648\n",
            "Epoch 86: loss improved from 0.25794 to 0.24367, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.2437 - accuracy: 0.9652 - val_loss: 2.3418 - val_accuracy: 0.3590\n",
            "Epoch 87/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2668 - accuracy: 0.9531\n",
            "Epoch 87: loss did not improve from 0.24367\n",
            "3/3 [==============================] - 0s 63ms/step - loss: 0.2594 - accuracy: 0.9565 - val_loss: 2.3972 - val_accuracy: 0.3590\n",
            "Epoch 88/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9536\n",
            "Epoch 88: loss improved from 0.24367 to 0.23635, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 0.2363 - accuracy: 0.9536 - val_loss: 2.4450 - val_accuracy: 0.3333\n",
            "Epoch 89/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.9536\n",
            "Epoch 89: loss did not improve from 0.23635\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 0.2456 - accuracy: 0.9536 - val_loss: 2.3884 - val_accuracy: 0.3077\n",
            "Epoch 90/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9739\n",
            "Epoch 90: loss improved from 0.23635 to 0.22115, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.2211 - accuracy: 0.9739 - val_loss: 2.3563 - val_accuracy: 0.3333\n",
            "Epoch 91/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9681\n",
            "Epoch 91: loss improved from 0.22115 to 0.21651, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2165 - accuracy: 0.9681 - val_loss: 2.4335 - val_accuracy: 0.3077\n",
            "Epoch 92/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.9681\n",
            "Epoch 92: loss improved from 0.21651 to 0.21635, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 72ms/step - loss: 0.2164 - accuracy: 0.9681 - val_loss: 2.3449 - val_accuracy: 0.3333\n",
            "Epoch 93/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.9507\n",
            "Epoch 93: loss did not improve from 0.21635\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.2446 - accuracy: 0.9507 - val_loss: 2.2698 - val_accuracy: 0.3077\n",
            "Epoch 94/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.9594\n",
            "Epoch 94: loss did not improve from 0.21635\n",
            "3/3 [==============================] - 0s 65ms/step - loss: 0.2266 - accuracy: 0.9594 - val_loss: 2.3443 - val_accuracy: 0.2821\n",
            "Epoch 95/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9536\n",
            "Epoch 95: loss did not improve from 0.21635\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 0.2217 - accuracy: 0.9536 - val_loss: 2.3964 - val_accuracy: 0.3333\n",
            "Epoch 96/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9681\n",
            "Epoch 96: loss did not improve from 0.21635\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 0.2181 - accuracy: 0.9681 - val_loss: 2.3878 - val_accuracy: 0.3590\n",
            "Epoch 97/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2380 - accuracy: 0.9570\n",
            "Epoch 97: loss did not improve from 0.21635\n",
            "3/3 [==============================] - 0s 73ms/step - loss: 0.2246 - accuracy: 0.9594 - val_loss: 2.3385 - val_accuracy: 0.3590\n",
            "Epoch 98/100\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.2259 - accuracy: 0.9648\n",
            "Epoch 98: loss improved from 0.21635 to 0.21166, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.2117 - accuracy: 0.9681 - val_loss: 2.3384 - val_accuracy: 0.3846\n",
            "Epoch 99/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.1979 - accuracy: 0.9739\n",
            "Epoch 99: loss improved from 0.21166 to 0.19786, saving model to best_model.h5\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 0.1979 - accuracy: 0.9739 - val_loss: 2.3339 - val_accuracy: 0.3590\n",
            "Epoch 100/100\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9681\n",
            "Epoch 100: loss did not improve from 0.19786\n",
            "3/3 [==============================] - 0s 57ms/step - loss: 0.2075 - accuracy: 0.9681 - val_loss: 2.3547 - val_accuracy: 0.3846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Summary\n",
        "#In this assignment, the focus was on optimizing a model (specify type, like LSTM, CNN, etc.) for a specific task. The data was carefully prepared and processed, with the model trained over 100 epochs, using a defined batch size and evaluation metrics like accuracy and loss. Initially, the model showed incremental improvements in accuracy, but faced challenges like potential overfitting as training progressed. The final performance indicated a respectable improvement in accuracy, though there were fluctuations in validation loss, suggesting areas for future optimization.\n",
        "\n",
        "#Reflecting on the process, certain strategies like specific architectural choices and data preprocessing techniques worked well, contributing to the steady increase in accuracy. However, some methods did not yield expected results, possibly due to the model's complexity or data characteristics. This experience highlighted the importance of continuous monitoring and adjustment during training, and the need for further exploration in areas like parameter tuning and model architecture for enhanced performance."
      ],
      "metadata": {
        "id": "HjMkENiUOE7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Extra Credit\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([raw_text])\n",
        "encoded = tokenizer.texts_to_sequences([raw_text])[0]\n",
        "\n",
        "sequence_length = 9\n",
        "sequences = []\n",
        "for i in range(sequence_length, len(encoded)):\n",
        "    sequence = encoded[i - sequence_length:i + 1]\n",
        "    sequences.append(sequence)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=9))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "model.fit(X, y, epochs=100, batch_size=128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bjyxaHI5NuXd",
        "outputId": "09cb5045-9390-49bd-9f32-8455dfed7dc5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 9, 50)             15050     \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 9, 100)            60400     \n",
            "                                                                 \n",
            " lstm_16 (LSTM)              (None, 100)               80400     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 301)               30401     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 196351 (767.00 KB)\n",
            "Trainable params: 196351 (767.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "9/9 [==============================] - 4s 40ms/step - loss: 5.6994 - accuracy: 0.0334\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.6204 - accuracy: 0.0382\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 5.1551 - accuracy: 0.0392\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 4.9443 - accuracy: 0.0697\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 1s 63ms/step - loss: 4.8552 - accuracy: 0.0697\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 4.7947 - accuracy: 0.0697\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 4.7561 - accuracy: 0.0497\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 4.7180 - accuracy: 0.0821\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 4.6906 - accuracy: 0.0707\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 1s 71ms/step - loss: 4.6372 - accuracy: 0.0860\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 1s 74ms/step - loss: 4.5694 - accuracy: 0.0888\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 4.4852 - accuracy: 0.1127\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 4.3838 - accuracy: 0.1347\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 4.2878 - accuracy: 0.1337\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 4.1940 - accuracy: 0.1471\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 4.0989 - accuracy: 0.1471\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 4.0021 - accuracy: 0.1614\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 3.9241 - accuracy: 0.1671\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 3.8505 - accuracy: 0.1757\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 3.7946 - accuracy: 0.1738\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 3.7215 - accuracy: 0.1843\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 3.6588 - accuracy: 0.1815\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 3.5662 - accuracy: 0.2063\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 3.4871 - accuracy: 0.2101\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 3.4336 - accuracy: 0.2178\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 3.3733 - accuracy: 0.2053\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 3.3092 - accuracy: 0.2321\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 3.2270 - accuracy: 0.2455\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 3.1550 - accuracy: 0.2550\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 3.1113 - accuracy: 0.2550\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 3.0298 - accuracy: 0.2741\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 47ms/step - loss: 2.9812 - accuracy: 0.2779\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 2.9200 - accuracy: 0.2846\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 2.8622 - accuracy: 0.3009\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 2.8126 - accuracy: 0.3142\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 2.7777 - accuracy: 0.3056\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 2.7701 - accuracy: 0.2980\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 1s 69ms/step - loss: 2.6884 - accuracy: 0.3161\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 2.6376 - accuracy: 0.3305\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 2.6013 - accuracy: 0.3391\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 1s 77ms/step - loss: 2.5596 - accuracy: 0.3410\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 2.5105 - accuracy: 0.3515\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 2.4791 - accuracy: 0.3563\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 51ms/step - loss: 2.4322 - accuracy: 0.3639\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 2.4243 - accuracy: 0.3496\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 2.3704 - accuracy: 0.3801\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 2.3361 - accuracy: 0.3773\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 2.2833 - accuracy: 0.3897\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 2.2709 - accuracy: 0.3983\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 2.2300 - accuracy: 0.3916\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 2.1827 - accuracy: 0.3945\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 2.1544 - accuracy: 0.4050\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 2.1483 - accuracy: 0.4002\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 2.1114 - accuracy: 0.4031\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 2.0801 - accuracy: 0.4126\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 2.0472 - accuracy: 0.4241\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 2.0448 - accuracy: 0.4374\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 2.0069 - accuracy: 0.4394\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 1.9796 - accuracy: 0.4508\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 1.9388 - accuracy: 0.4537\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 1.9025 - accuracy: 0.4575\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 1.8767 - accuracy: 0.4728\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 1.8538 - accuracy: 0.4766\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 1.8440 - accuracy: 0.4823\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 1.8050 - accuracy: 0.4900\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 1.7647 - accuracy: 0.4986\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 1.7361 - accuracy: 0.5005\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 1.7102 - accuracy: 0.4957\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 1.7097 - accuracy: 0.5100\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 1.6820 - accuracy: 0.5119\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 1.6692 - accuracy: 0.5205\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 1s 75ms/step - loss: 1.6563 - accuracy: 0.5062\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 1.6166 - accuracy: 0.5186\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 1s 70ms/step - loss: 1.6003 - accuracy: 0.5339\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 1.5931 - accuracy: 0.5310\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 1.5682 - accuracy: 0.5234\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 1.5338 - accuracy: 0.5368\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 1.5006 - accuracy: 0.5654\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 1.5087 - accuracy: 0.5654\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 1.5261 - accuracy: 0.5425\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 1.4785 - accuracy: 0.5731\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 1.4456 - accuracy: 0.5817\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 1.4419 - accuracy: 0.5817\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 1.4526 - accuracy: 0.5606\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 1.4196 - accuracy: 0.5702\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 1.3835 - accuracy: 0.5979\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 1.3640 - accuracy: 0.5903\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 1.3314 - accuracy: 0.6055\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 1.3593 - accuracy: 0.6008\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 1.3387 - accuracy: 0.6036\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 1.3274 - accuracy: 0.6084\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 1.2700 - accuracy: 0.6208\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 1.2557 - accuracy: 0.6371\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 1.2374 - accuracy: 0.6504\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 1.2098 - accuracy: 0.6447\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 1.2039 - accuracy: 0.6361\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 1.2087 - accuracy: 0.6457\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 1.1788 - accuracy: 0.6609\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 1.1753 - accuracy: 0.6485\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 1.1874 - accuracy: 0.6514\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c16af42b460>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6eEEuswAPf_0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}